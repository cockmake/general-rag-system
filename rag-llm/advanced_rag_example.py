"""
LangChainé«˜çº§RAGæ£€ç´¢ç¤ºä¾‹ - å¤šè§’åº¦æŸ¥è¯¢ä¸å¹¶è¡Œå¤„ç†
åŒ…å«ä»¥ä¸‹é«˜çº§ç‰¹æ€§ï¼š
1. å¤šè§’åº¦æŸ¥è¯¢ç”Ÿæˆ(Multi-Query Generation) - ä½¿ç”¨LLMç”Ÿæˆå¤šä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢
2. å¹¶è¡Œæ£€ç´¢(Parallel Retrieval) - åŒæ—¶æ£€ç´¢å¤šä¸ªæŸ¥è¯¢å¹¶å»é‡
3. å¹¶è¡Œæ–‡æ¡£è¯„åˆ†(Parallel Document Grading) - å¹¶è¡Œè¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§å¹¶æ‰“åˆ†
4. å¯¹è¯å†å²ç®¡ç† - æ”¯æŒå¤šè½®å¯¹è¯ï¼ˆæœ€è¿‘5è½®ï¼‰ï¼Œç†è§£ä¸Šä¸‹æ–‡
5. æ™ºèƒ½ç­”æ¡ˆç”Ÿæˆ - åŸºäºæ£€ç´¢ç»“æœå’Œå†å²ç”Ÿæˆç­”æ¡ˆ

æµç¨‹ï¼š
1. æ ¹æ®ç”¨æˆ·è¾“å…¥ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢ï¼ˆè°ƒç”¨LLMï¼‰
2. å¹¶è¡Œæ£€ç´¢æ‰€æœ‰æŸ¥è¯¢å¹¶æ±‡æ€»å»é‡
3. å¹¶è¡Œå¯¹æ–‡æ¡£è¿›è¡Œç›¸å…³æ€§è¯„åˆ†
4. åŸºäºç›¸å…³æ–‡æ¡£å’Œå¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®ï¼‰ç”Ÿæˆç­”æ¡ˆ
"""
from typing import Literal

from langchain.chat_models import init_chat_model
from langchain.embeddings import init_embeddings
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.vectorstores import InMemoryVectorStore
from langgraph.graph import StateGraph, START, END, MessagesState
from pydantic import BaseModel, Field

from utils import get_llm_instance

# ============= 1. æ„å»ºçŸ¥è¯†åº“ =============
# åˆ›å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨
# embeddings = init_embeddings(
#     model="text-embedding-v4",
#     api_key="sk-188e60cd3e844cab97bc30138dac5cd7",
#     base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
#     provider="openai"
# )
embeddings = init_embeddings(
    model="text-embedding-v4",
    api_key="sk-188e60cd3e844cab97bc30138dac5cd7",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    check_embedding_ctx_length=False,
    dimensions=1536,
    provider="openai"
)
vector_store = InMemoryVectorStore(embeddings)

# æ·»åŠ ç¤ºä¾‹æ–‡æ¡£ - å…³äºæœºå™¨å­¦ä¹ çš„çŸ¥è¯†
documents = [
    # ç›‘ç£å­¦ä¹ ç›¸å…³
    "ç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨å¸¦æ ‡ç­¾çš„è®­ç»ƒæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚å¸¸è§ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—å’Œæ”¯æŒå‘é‡æœº(SVM)ã€‚",
    "çº¿æ€§å›å½’ç”¨äºé¢„æµ‹è¿ç»­å€¼ï¼Œè€Œé€»è¾‘å›å½’ç”¨äºåˆ†ç±»é—®é¢˜ã€‚ä¸¤è€…éƒ½æ˜¯ç›‘ç£å­¦ä¹ çš„åŸºç¡€ç®—æ³•ã€‚",
    "å†³ç­–æ ‘é€šè¿‡æ ‘çŠ¶ç»“æ„è¿›è¡Œå†³ç­–ï¼Œéšæœºæ£®æ—æ˜¯å¤šä¸ªå†³ç­–æ ‘çš„é›†æˆï¼Œå¯ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§å¹¶å‡å°‘è¿‡æ‹Ÿåˆã€‚",

    # æ— ç›‘ç£å­¦ä¹ ç›¸å…³
    "æ— ç›‘ç£å­¦ä¹ å¤„ç†æ²¡æœ‰æ ‡ç­¾çš„æ•°æ®ï¼Œä¸»è¦ç”¨äºèšç±»å’Œé™ç»´ã€‚å¸¸è§ç®—æ³•åŒ…æ‹¬K-meansèšç±»ã€å±‚æ¬¡èšç±»ã€PCAä¸»æˆåˆ†åˆ†æå’Œt-SNEã€‚",
    "K-meansæ˜¯æœ€å¸¸ç”¨çš„èšç±»ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£å°†æ•°æ®ç‚¹åˆ†é…åˆ°Kä¸ªç°‡ä¸­ã€‚å±‚æ¬¡èšç±»åˆ™æ„å»ºæ ‘çŠ¶çš„ç°‡ç»“æ„ã€‚",
    "PCAä¸»æˆåˆ†åˆ†æç”¨äºé™ç»´ï¼Œä¿ç•™æ•°æ®ä¸­æœ€é‡è¦çš„ç‰¹å¾ã€‚t-SNEåˆ™å¸¸ç”¨äºé«˜ç»´æ•°æ®çš„å¯è§†åŒ–ã€‚",

    # æ·±åº¦å­¦ä¹ ç›¸å…³
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚å¸¸è§æ¶æ„åŒ…æ‹¬CNN(å·ç§¯ç¥ç»ç½‘ç»œ)ã€RNN(å¾ªç¯ç¥ç»ç½‘ç»œ)å’ŒTransformerã€‚",
    "CNNç‰¹åˆ«é€‚åˆå¤„ç†å›¾åƒæ•°æ®ï¼Œé€šè¿‡å·ç§¯å±‚æå–ç©ºé—´ç‰¹å¾ã€‚å¸¸ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ã€‚",
    "RNNå’ŒLSTMé€‚åˆå¤„ç†åºåˆ—æ•°æ®å¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚Transformeræ¶æ„åˆ™æ˜¯ç°ä»£NLPçš„åŸºç¡€ï¼Œå¦‚GPTå’ŒBERTæ¨¡å‹ã€‚",

    # å¼ºåŒ–å­¦ä¹ ç›¸å…³
    "å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚Agenté€šè¿‡è¯•é”™è·å¾—å¥–åŠ±ä¿¡å·ï¼Œé€æ­¥ä¼˜åŒ–è¡Œä¸ºç­–ç•¥ã€‚",
    "Q-learningå’ŒDeep Q-Network(DQN)æ˜¯å€¼å‡½æ•°æ–¹æ³•ï¼ŒPolicy Gradientå’ŒActor-Criticæ˜¯ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚",
    "å¼ºåŒ–å­¦ä¹ åº”ç”¨å¹¿æ³›ï¼ŒåŒ…æ‹¬æ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€æ¨èç³»ç»Ÿå’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚",
]
max_batch = 10
for i in range(0, len(documents), max_batch):
    batch_docs = documents[i:i + max_batch]
    vector_store.add_texts(batch_docs)

retriever = vector_store.as_retriever(
    search_kwargs={"k": 5}
)


# ============= 2. å®šä¹‰çŠ¶æ€ç»“æ„ =============
class RAGState(MessagesState):
    """RAGç³»ç»Ÿçš„çŠ¶æ€"""
    messages: list  # å¯¹è¯å†å²
    original_query: str  # åŸå§‹æŸ¥è¯¢
    query_list: list[str]  # å¤šè§’åº¦æŸ¥è¯¢åˆ—è¡¨
    documents: list[str]  # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    relevance_scores: list[bool]  # æ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†
    answer: str  # æœ€ç»ˆç­”æ¡ˆ


# ============= 3. å¤šè§’åº¦æŸ¥è¯¢ç”Ÿæˆæ¨¡å— =============
class MultiQueryList(BaseModel):
    """å¤šè§’åº¦æŸ¥è¯¢åˆ—è¡¨"""
    queries: list[str] = Field(description="ä»ä¸åŒè§’åº¦ç”Ÿæˆçš„æŸ¥è¯¢åˆ—è¡¨ï¼Œ3-5ä¸ªæŸ¥è¯¢")
    reasoning: str = Field(description="ç”Ÿæˆè¿™äº›æŸ¥è¯¢çš„åŸå› ")


def multi_query_generator_node(state: RAGState) -> dict:
    """
    å¤šè§’åº¦æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆå¤šä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢
    é«˜çº§ç‰¹æ€§ï¼š
    - è€ƒè™‘å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘5è½®ï¼‰
    - ä»ä¸åŒè¯­ä¹‰è§’åº¦ç†è§£é—®é¢˜
    - ç”Ÿæˆäº’è¡¥æ€§æŸ¥è¯¢ä»¥æé«˜å¬å›ç‡
    """
    messages = state["messages"]
    current_question = messages[-1].content

    # æ„å»ºå¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®ï¼Œå³10æ¡æ¶ˆæ¯ï¼‰
    history_context = ""
    if len(messages) > 1:
        recent_history = messages[-11:-1] if len(messages) > 10 else messages[:-1]
        history_context = "\n".join([
            f"{'ç”¨æˆ·' if isinstance(m, HumanMessage) else 'åŠ©æ‰‹'}: {m.content}"
            for m in recent_history
        ])

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œä»ä¸åŒè§’åº¦ç”Ÿæˆ3-5ä¸ªæŸ¥è¯¢ï¼Œä»¥ä¾¿å…¨é¢æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚

ç”Ÿæˆç­–ç•¥ï¼š
1. ç†è§£é—®é¢˜çš„æ ¸å¿ƒæ„å›¾ï¼Œç»“åˆå¯¹è¯å†å²è§£æä»£è¯å’Œä¸Šä¸‹æ–‡
2. ä»ä¸åŒè¯­ä¹‰è§’åº¦æ‹†è§£é—®é¢˜ï¼ˆå¦‚ï¼šå®šä¹‰ã€åº”ç”¨ã€å¯¹æ¯”ã€åŸç†ç­‰ï¼‰
3. ç”Ÿæˆçš„æŸ¥è¯¢åº”è¯¥äº’è¡¥ï¼Œè¦†ç›–é—®é¢˜çš„ä¸åŒæ–¹é¢
4. æ¯ä¸ªæŸ¥è¯¢åº”ç®€æ´æ˜ç¡®ï¼Œé€‚åˆå‘é‡æ£€ç´¢
5. æ‰©å±•ç›¸å…³æ¦‚å¿µå’ŒåŒä¹‰è¯

çŸ¥è¯†åº“ä¸»é¢˜ï¼šæœºå™¨å­¦ä¹ ç®—æ³•å’Œæ¦‚å¿µ

{'å¯¹è¯å†å²ï¼š\n' + history_context if history_context else 'æ— å¯¹è¯å†å²'}

å½“å‰é—®é¢˜ï¼š{current_question}

è¯·ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢ã€‚"""

    llm = get_llm_instance({
        "provider": "deepseek",
        "name": "deepseek-chat"
    }, temperature=0.5)
    structured_llm = llm.with_structured_output(MultiQueryList)

    result = structured_llm.invoke([{"role": "system", "content": system_prompt}])

    print(f"\nğŸ”„ å¤šè§’åº¦æŸ¥è¯¢ç”Ÿæˆ:")
    print(f"  åŸå§‹é—®é¢˜: {current_question}")
    print(f"  ç”ŸæˆåŸå› : {result.reasoning}")
    print(f"  ç”Ÿæˆçš„æŸ¥è¯¢åˆ—è¡¨:")
    for i, q in enumerate(result.queries, 1):
        print(f"    {i}. {q}")

    return {
        "original_query": current_question,
        "query_list": result.queries
    }


# ============= 4. å¹¶è¡Œæ£€ç´¢æ¨¡å— =============
def parallel_retrieval_node(state: RAGState) -> dict:
    """
    å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŸ¥è¯¢çš„å‘é‡æ£€ç´¢ï¼Œå¹¶å»é‡
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    query_list = state["query_list"]
    all_docs = []
    doc_set = set()  # ç”¨äºå»é‡

    print(f"\nğŸ“š å¹¶è¡Œæ£€ç´¢ {len(query_list)} ä¸ªæŸ¥è¯¢:")

    def retrieve_single_query(query: str, index: int):
        """å•ä¸ªæŸ¥è¯¢çš„æ£€ç´¢å‡½æ•°"""
        docs = retriever.invoke(query)
        doc_contents = [doc.page_content for doc in docs]
        print(f"  æŸ¥è¯¢{index}: æ£€ç´¢åˆ° {len(doc_contents)} ä¸ªæ–‡æ¡£")
        return doc_contents

    # å¹¶è¡Œæ£€ç´¢æ‰€æœ‰æŸ¥è¯¢
    with ThreadPoolExecutor(max_workers=len(query_list)) as executor:
        future_to_query = {
            executor.submit(retrieve_single_query, query, i + 1): query
            for i, query in enumerate(query_list)
        }

        for future in as_completed(future_to_query):
            try:
                doc_contents = future.result()
                # å»é‡ï¼šåªæ·»åŠ æœªè§è¿‡çš„æ–‡æ¡£
                for doc in doc_contents:
                    if doc not in doc_set:
                        doc_set.add(doc)
                        all_docs.append(doc)
            except Exception as e:
                print(f"  æ£€ç´¢å‡ºé”™: {str(e)}")

    print(f"\n  å»é‡åå…± {len(all_docs)} ä¸ªå”¯ä¸€æ–‡æ¡£")

    return {"documents": all_docs}


# ============= 5. å¹¶è¡Œæ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†æ¨¡å— =============
class RelevanceScore(BaseModel):
    """æ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†"""
    is_relevant: bool = Field(description="æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³")
    reason: str = Field(description="åˆ¤æ–­ç†ç”±")
    score: float = Field(description="ç›¸å…³æ€§åˆ†æ•°ï¼Œ0-1ä¹‹é—´", ge=0, le=1)


def parallel_grade_documents_node(state: RAGState) -> dict:
    """
    å¹¶è¡Œè¯„ä¼°æ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§å¹¶æ‰“åˆ†
    é«˜çº§ç‰¹æ€§ï¼šä½¿ç”¨LLMå¹¶è¡Œåˆ¤æ–­æ–‡æ¡£æ˜¯å¦çœŸæ­£å›ç­”ç”¨æˆ·é—®é¢˜
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    query = state["original_query"]
    documents = state["documents"]

    print(f"\nâš–ï¸ å¹¶è¡Œæ–‡æ¡£ç›¸å…³æ€§è¯„åˆ† ({len(documents)} ä¸ªæ–‡æ¡£):")

    # å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œç›´æ¥è¿”å›
    if not documents:
        print("  æ— æ–‡æ¡£éœ€è¦è¯„åˆ†")
        return {
            "relevance_scores": [],
            "documents": []
        }

    llm = get_llm_instance({
        "provider": "deepseek",
        "name": "deepseek-chat"
    }, temperature=0)
    structured_llm = llm.with_structured_output(RelevanceScore)

    def grade_single_document(doc: str, index: int):
        """å•ä¸ªæ–‡æ¡£çš„è¯„åˆ†å‡½æ•°"""
        prompt = f"""è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£æ˜¯å¦ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³ï¼Œå¹¶ç»™å‡ºç›¸å…³æ€§åˆ†æ•°ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

æ–‡æ¡£å†…å®¹ï¼š{doc}

åˆ¤æ–­æ ‡å‡†ï¼š
1. æ–‡æ¡£æ˜¯å¦ç›´æ¥å›ç­”é—®é¢˜
2. æ–‡æ¡£æ˜¯å¦åŒ…å«é—®é¢˜ä¸­æåˆ°çš„æ¦‚å¿µ
3. æ–‡æ¡£ä¿¡æ¯æ˜¯å¦æœ‰åŠ©äºå›ç­”é—®é¢˜

è¯·ä¸¥æ ¼è¯„åˆ¤ï¼Œåªæœ‰çœŸæ­£ç›¸å…³çš„æ–‡æ¡£æ‰æ ‡è®°ä¸ºç›¸å…³ï¼Œå¹¶ç»™å‡º0-1ä¹‹é—´çš„ç›¸å…³æ€§åˆ†æ•°ã€‚"""

        score = structured_llm.invoke([{"role": "user", "content": prompt}])
        return (index, doc, score)

    # å¹¶è¡Œè¯„åˆ†æ‰€æœ‰æ–‡æ¡£
    doc_scores = []
    with ThreadPoolExecutor(max_workers=min(len(documents), 5)) as executor:
        future_to_doc = {
            executor.submit(grade_single_document, doc, i + 1): (i, doc)
            for i, doc in enumerate(documents)
        }

        for future in as_completed(future_to_doc):
            try:
                index, doc, score = future.result()
                doc_scores.append((index, doc, score))
                print(
                    f"  æ–‡æ¡£{index}: {'âœ“ ç›¸å…³' if score.is_relevant else 'âœ— ä¸ç›¸å…³'} (åˆ†æ•°: {score.score:.2f}) - {score.reason}")
            except Exception as e:
                print(f"  è¯„åˆ†å‡ºé”™: {str(e)}")

    # æŒ‰åŸå§‹é¡ºåºæ’åº
    doc_scores.sort(key=lambda x: x[0])

    # æå–ç›¸å…³æ–‡æ¡£ï¼ˆæŒ‰åˆ†æ•°æ’åºï¼‰
    relevant_docs_with_scores = [(doc, score.score) for _, doc, score in doc_scores if score.is_relevant]
    relevant_docs_with_scores.sort(key=lambda x: x[1], reverse=True)

    relevant_docs = [doc for doc, _ in relevant_docs_with_scores]
    relevance_scores = [score.is_relevant for _, _, score in doc_scores]

    print(f"\n  å…± {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼ˆå·²æŒ‰åˆ†æ•°æ’åºï¼‰")

    return {
        "relevance_scores": relevance_scores,
        "documents": relevant_docs  # åªä¿ç•™ç›¸å…³æ–‡æ¡£ï¼ŒæŒ‰åˆ†æ•°é™åº
    }


# ============= 6. å†³ç­–è·¯ç”± =============
def should_generate(state: RAGState) -> Literal["generate", "end"]:
    """
    å†³å®šæ˜¯å¦ç”Ÿæˆç­”æ¡ˆ
    ç”±äºå·²ç»ç”Ÿæˆäº†å¤šè§’åº¦æŸ¥è¯¢ï¼Œå¦‚æœæ²¡æœ‰ç›¸å…³æ–‡æ¡£åˆ™ç›´æ¥ç»“æŸï¼Œä¸å†é‡è¯•
    """
    relevance_scores = state.get("relevance_scores", [])

    return "generate"


# ============= 7. ç­”æ¡ˆç”Ÿæˆæ¨¡å— =============
def generate_answer_node(state: RAGState) -> dict:
    """
    åŸºäºæ£€ç´¢æ–‡æ¡£å’Œå¯¹è¯å†å²ç”Ÿæˆç­”æ¡ˆ
    é«˜çº§ç‰¹æ€§ï¼š
    - å¼•ç”¨æ¥æºæ–‡æ¡£
    - ä¿æŒå¯¹è¯è¿è´¯æ€§ï¼ˆåŒ…å«æœ€è¿‘5è½®å¯¹è¯ï¼‰
    - å¦‚æœæ–‡æ¡£ä¸è¶³ä»¥å›ç­”ï¼Œæ˜ç¡®è¯´æ˜
    """
    messages = state["messages"]
    documents = state["documents"]
    query = state["original_query"]

    # æ„å»ºä¸Šä¸‹æ–‡
    if documents:
        context = "\n\n".join([f"[æ–‡æ¡£{i + 1}] {doc}" for i, doc in enumerate(documents)])
    else:
        context = "æ— ç›¸å…³æ–‡æ¡£"

    # è·å–å¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®ï¼Œå³10æ¡æ¶ˆæ¯ï¼‰
    history_messages = messages[:-1]  # ä¸åŒ…æ‹¬å½“å‰é—®é¢˜
    recent_history = history_messages[-10:] if len(history_messages) > 10 else history_messages

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœºå™¨å­¦ä¹ åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„æ–‡æ¡£å’Œå¯¹è¯å†å²å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ä¼˜å…ˆä½¿ç”¨æä¾›çš„æ–‡æ¡£ä¸­çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸è¶³ä»¥å®Œæ•´å›ç­”ï¼Œç»“åˆå¯¹è¯å†å²è¿›è¡Œæ¨ç†æˆ–æ˜ç¡®è¯´æ˜
3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç¼–å·ï¼ˆå¦‚æœ‰ï¼‰
4. ä¿æŒå¯¹è¯è¿è´¯ï¼Œè€ƒè™‘å†å²ä¸Šä¸‹æ–‡
5. ç”¨æ¸…æ™°ã€ç®€æ´çš„è¯­è¨€å›ç­”

å‚è€ƒæ–‡æ¡£ï¼š
{context}"""

    conversation = [{"role": "system", "content": system_prompt}]

    # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘5è½®ï¼‰
    for msg in recent_history:
        if isinstance(msg, HumanMessage):
            conversation.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            conversation.append({"role": "assistant", "content": msg.content})

    # æ·»åŠ å½“å‰é—®é¢˜
    conversation.append({"role": "user", "content": query})

    llm = init_chat_model(
        model="deepseek-chat",
        api_key="sk-e34cc6e3056045bea1da92160035e0df",
        base_url="https://api.deepseek.com/v1",
    )
    response = llm.invoke(conversation)

    print(f"\nğŸ’¡ ç”Ÿæˆç­”æ¡ˆ:")
    print(f"  {response.content}")

    return {"answer": response.content}


# ============= 8. æ„å»ºRAGå·¥ä½œæµ =============
def build_rag_workflow():
    """
    æ„å»ºå®Œæ•´çš„RAGå·¥ä½œæµ
    æµç¨‹ï¼šå¤šè§’åº¦æŸ¥è¯¢ç”Ÿæˆ -> å¹¶è¡Œæ£€ç´¢ -> å¹¶è¡Œè¯„åˆ† -> ç”Ÿæˆç­”æ¡ˆ
    """
    workflow = StateGraph(RAGState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("multi_query", multi_query_generator_node)
    workflow.add_node("parallel_retrieve", parallel_retrieval_node)
    workflow.add_node("parallel_grade", parallel_grade_documents_node)
    workflow.add_node("generate", generate_answer_node)

    # æ·»åŠ è¾¹ï¼šçº¿æ€§æµç¨‹ï¼Œä¸å†å¾ªç¯é‡è¯•
    workflow.add_edge(START, "multi_query")
    workflow.add_edge("multi_query", "parallel_retrieve")
    workflow.add_edge("parallel_retrieve", "parallel_grade")

    # æ¡ä»¶è·¯ç”±ï¼šæ ¹æ®æ–‡æ¡£ç›¸å…³æ€§å†³å®šä¸‹ä¸€æ­¥
    workflow.add_conditional_edges(
        "parallel_grade",
        should_generate,
        {
            "generate": "generate",
            "end": END
        }
    )

    workflow.add_edge("generate", END)

    return workflow.compile()


# ============= 9. å¯¹è¯å¼RAGç±» =============
class ConversationalRAG:
    """æ”¯æŒå¤šè½®å¯¹è¯çš„RAGç³»ç»Ÿ"""

    def __init__(self):
        self.workflow = build_rag_workflow()
        self.conversation_history = []

    def ask(self, question: str) -> str:
        """æé—®å¹¶è·å–ç­”æ¡ˆ"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.conversation_history.append(HumanMessage(content=question))

        # æ‰§è¡ŒRAGå·¥ä½œæµ
        state = {
            "messages": self.conversation_history,
            "original_query": "",
            "query_list": [],
            "documents": [],
            "relevance_scores": [],
            "answer": ""
        }

        result = self.workflow.invoke(state)
        answer = result["answer"]

        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        self.conversation_history.append(AIMessage(content=answer))

        return answer

    def reset(self):
        """é‡ç½®å¯¹è¯å†å²"""
        self.conversation_history = []
        print("\nğŸ”„ å¯¹è¯å†å²å·²æ¸…ç©º")


# ============= 10. ä½¿ç”¨ç¤ºä¾‹ =============
if __name__ == "__main__":
    print("=" * 80)
    print("LangChainé«˜çº§RAGæ£€ç´¢ç¤ºä¾‹ - å¯¹è¯å¼é—®ç­”ç³»ç»Ÿ")
    print("=" * 80)

    # åˆ›å»ºå¯¹è¯å¼RAGç³»ç»Ÿ
    rag = ConversationalRAG()

    # ç¤ºä¾‹å¯¹è¯1ï¼šåŸºç¡€é—®ç­”
    print("\n\nã€å¯¹è¯è½®æ¬¡ 1ã€‘")
    print("-" * 80)
    question1 = "ä»€ä¹ˆæ˜¯ç›‘ç£å­¦ä¹ ï¼Ÿ"
    print(f"ğŸ‘¤ ç”¨æˆ·: {question1}")
    answer1 = rag.ask(question1)
    print(f"ğŸ¤– åŠ©æ‰‹: {answer1}")

    # ç¤ºä¾‹å¯¹è¯2ï¼šä¸Šä¸‹æ–‡ç†è§£ï¼ˆä½¿ç”¨ä»£è¯ï¼‰
    print("\n\nã€å¯¹è¯è½®æ¬¡ 2ã€‘")
    print("-" * 80)
    question2 = "å®ƒæœ‰å“ªäº›å¸¸è§ç®—æ³•ï¼Ÿ"  # "å®ƒ"æŒ‡ä»£"ç›‘ç£å­¦ä¹ "
    print(f"ğŸ‘¤ ç”¨æˆ·: {question2}")
    answer2 = rag.ask(question2)
    print(f"ğŸ¤– åŠ©æ‰‹: {answer2}")

    # ç¤ºä¾‹å¯¹è¯3ï¼šæ·±å…¥è¿½é—®
    print("\n\nã€å¯¹è¯è½®æ¬¡ 3ã€‘")
    print("-" * 80)
    question3 = "çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    print(f"ğŸ‘¤ ç”¨æˆ·: {question3}")
    answer3 = rag.ask(question3)
    print(f"ğŸ¤– åŠ©æ‰‹: {answer3}")

    # ç¤ºä¾‹å¯¹è¯4ï¼šåˆ‡æ¢è¯é¢˜
    print("\n\nã€å¯¹è¯è½®æ¬¡ 4ã€‘")
    print("-" * 80)
    question4 = "æ·±åº¦å­¦ä¹ ä¸­çš„CNNä¸»è¦ç”¨äºä»€ä¹ˆï¼Ÿ"
    print(f"ğŸ‘¤ ç”¨æˆ·: {question4}")
    answer4 = rag.ask(question4)
    print(f"ğŸ¤– åŠ©æ‰‹: {answer4}")

    # ç¤ºä¾‹å¯¹è¯5ï¼šå¯¹æ¯”é—®é¢˜ï¼ˆä¼šè§¦å‘æŸ¥è¯¢é‡å†™ï¼‰
    print("\n\nã€å¯¹è¯è½®æ¬¡ 5ã€‘")
    print("-" * 80)
    question5 = "ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"ğŸ‘¤ ç”¨æˆ·: {question5}")
    answer5 = rag.ask(question5)
    print(f"ğŸ¤– åŠ©æ‰‹: {answer5}")

    print("\n\n" + "=" * 80)
    print("å¯¹è¯ç¤ºä¾‹ç»“æŸ")
    print("=" * 80)

    # æ‰“å°é«˜çº§ç‰¹æ€§è¯´æ˜
    print("\n\nğŸ“– æœ¬ç¤ºä¾‹å±•ç¤ºçš„é«˜çº§RAGç‰¹æ€§ï¼š")
    print("-" * 80)
    print("""
1. ã€å¤šè§’åº¦æŸ¥è¯¢ç”Ÿæˆ (Multi-Query Generation)ã€‘
   - æ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆ3-5ä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢
   - ç»“åˆå¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®ï¼‰è§£æä¸Šä¸‹æ–‡
   - ä»ä¸åŒè¯­ä¹‰è§’åº¦æé«˜å¬å›ç‡
   
2. ã€å¹¶è¡Œæ£€ç´¢ (Parallel Retrieval)ã€‘
   - åŒæ—¶å¯¹å¤šä¸ªæŸ¥è¯¢è¿›è¡Œå‘é‡æ£€ç´¢
   - è‡ªåŠ¨å»é‡ï¼Œæ±‡æ€»æ‰€æœ‰æ£€ç´¢ç»“æœ
   - æé«˜æ£€ç´¢æ•ˆç‡å’Œè¦†ç›–ç‡
   
3. ã€å¹¶è¡Œæ–‡æ¡£è¯„åˆ† (Parallel Document Grading)ã€‘
   - å¹¶è¡Œè¯„ä¼°æ‰€æœ‰æ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§
   - ä½¿ç”¨LLMç»™å‡º0-1çš„ç›¸å…³æ€§åˆ†æ•°
   - æŒ‰åˆ†æ•°æ’åºï¼Œè¿‡æ»¤ä¸ç›¸å…³æ–‡æ¡£
   
4. ã€å¯¹è¯å†å²ç®¡ç†ã€‘
   - ç»´æŠ¤å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘5è½®ï¼‰
   - ç†è§£ä»£è¯æŒ‡ä»£ï¼ˆå¦‚"å®ƒ"ã€"è¿™ä¸ª"ï¼‰
   - ä¿æŒå¯¹è¯è¿è´¯æ€§
   
5. ã€æ™ºèƒ½ç­”æ¡ˆç”Ÿæˆã€‘
   - åŸºäºç›¸å…³æ–‡æ¡£å’Œå¯¹è¯å†å²ç”Ÿæˆç­”æ¡ˆ
   - å¼•ç”¨æ–‡æ¡£æ¥æºç¼–å·
   - å¦‚æ— ç›¸å…³æ–‡æ¡£åˆ™æ˜ç¡®è¯´æ˜
    """)

    print("\nğŸ’¡ å®è·µå»ºè®®ï¼š")
    print("-" * 80)
    print("""
- æ ¹æ®å®é™…åœºæ™¯è°ƒæ•´ç”Ÿæˆçš„æŸ¥è¯¢æ•°é‡å’Œæ£€ç´¢çš„ k å€¼
- ä¼˜åŒ–å‘é‡æ¨¡å‹é€‰æ‹©ï¼ˆå¦‚ text-embedding-3-largeï¼‰
- å®ç°æ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†çš„ç¼“å­˜æœºåˆ¶
- æ·»åŠ å¯¹è¯å†å²æ‘˜è¦åŠŸèƒ½ï¼ˆé•¿å¯¹è¯åœºæ™¯ï¼‰
- è°ƒæ•´å¹¶è¡Œçº¿ç¨‹æ•°ä»¥å¹³è¡¡é€Ÿåº¦å’Œèµ„æºæ¶ˆè€—
- å¯é€‰ï¼šä½¿ç”¨é‡æ’åºæ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–æ£€ç´¢ç»“æœ
- å¤šè§’åº¦æŸ¥è¯¢å·²è¦†ç›–å¤šæ–¹é¢ï¼Œæ— éœ€é¢å¤–é‡è¯•æœºåˆ¶
    """)
