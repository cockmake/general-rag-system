# Embedding Service å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“¦ å®‰è£…

### 1. å®‰è£…Pythonä¾èµ–

```bash
cd embedding_rerank
pip install -r requirements.txt
```

**æ³¨æ„**ï¼šéœ€è¦ Python 3.8+ï¼Œæ¨èä½¿ç”¨è™šæ‹Ÿç¯å¢ƒã€‚

### 2. é…ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰

```bash
# å¤åˆ¶é…ç½®æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œä¿®æ”¹é…ç½®
# å¤§å¤šæ•°æƒ…å†µä¸‹ä½¿ç”¨é»˜è®¤é…ç½®å³å¯
```

## ğŸš€ å¯åŠ¨æœåŠ¡

### æ–¹å¼ä¸€ï¼šä½¿ç”¨Pythonç›´æ¥å¯åŠ¨

```bash
python start.py
```

### æ–¹å¼äºŒï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬

**Linux/Mac:**
```bash
bash run.sh
```

**Windows:**
```cmd
run.bat
```

### å¯åŠ¨æˆåŠŸæ ‡å¿—

çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºè¯´æ˜æœåŠ¡å¯åŠ¨æˆåŠŸï¼š

```
======================================================================
Embedding Service Configuration:
======================================================================
Model: Qwen/Qwen3-Embedding-0.6B
GPU Memory Utilization: 0.4
Max Model Length: 3072
...
======================================================================
Embedding Service Ready!
======================================================================
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8890
```

## ğŸ§ª æµ‹è¯•æœåŠ¡

### 1. è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•

```bash
python test_service.py
```

### 2. æ‰‹åŠ¨æµ‹è¯•

**å¥åº·æ£€æŸ¥:**
```bash
curl http://localhost:8890/health
```

**ç”Ÿæˆå‘é‡:**
```bash
curl -X POST http://localhost:8890/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{"input": "æµ‹è¯•æ–‡æœ¬"}'
```

## ğŸ“Š å¸¸ç”¨é…ç½®

### æ˜¾å­˜å ç”¨è°ƒæ•´

é»˜è®¤å ç”¨ 40% GPU æ˜¾å­˜ï¼Œå¦‚éœ€è°ƒæ•´ï¼š

```bash
# æ–¹å¼1: ç¯å¢ƒå˜é‡
export EMBEDDING_GPU_MEMORY_UTILIZATION=0.6

# æ–¹å¼2: ä¿®æ”¹ .env æ–‡ä»¶
EMBEDDING_GPU_MEMORY_UTILIZATION=0.6
```

### ç«¯å£ä¿®æ”¹

```bash
# ç¯å¢ƒå˜é‡
export EMBEDDING_PORT=9000

# æˆ–ä¿®æ”¹ .env æ–‡ä»¶
EMBEDDING_PORT=9000
```

### æ›´æ¢æ¨¡å‹

```bash
# ä½¿ç”¨å…¶ä»–æ¨¡å‹
export EMBEDDING_MODEL_NAME=BAAI/bge-large-zh-v1.5

# æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
export EMBEDDING_MODEL_PATH=/path/to/local/model
```

## ğŸ”Œ é›†æˆåˆ° rag-llm

### 1. ä¿®æ”¹ rag-llm é…ç½®

åœ¨ `rag-llm/model_config.json` ä¸­æ·»åŠ ï¼š

```json
{
  "embedding": {
    "local": {
      "settings": {
        "base_url": "http://localhost:8890/v1",
        "api_key": "dummy",
        "provider": "openai",
        "dimensions": 1024
      },
      "text-embedding-0.6b": {}
    }
  }
}
```

### 2. ä½¿ç”¨æœ¬åœ°Embedding

åœ¨ `rag-llm` ä»£ç ä¸­ï¼š

```python
# åŸæ¥ä½¿ç”¨çš„é…ç½®
embedding_config = {
    'name': 'text-embedding-v4',
    'provider': 'qwen'
}

# æ”¹ä¸ºä½¿ç”¨æœ¬åœ°æœåŠ¡
embedding_config = {
    'name': 'text-embedding-0.6b',
    'provider': 'local'
}

embeddings = get_embedding_instance(embedding_config)
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. ç«¯å£è¢«å ç”¨

**é”™è¯¯**: `Address already in use`

**è§£å†³**:
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
netstat -ano | findstr :8890  # Windows
lsof -i :8890                 # Linux/Mac

# æˆ–è€…æ›´æ¢ç«¯å£
export EMBEDDING_PORT=8891
```

### 2. æ˜¾å­˜ä¸è¶³

**é”™è¯¯**: `CUDA out of memory`

**è§£å†³**:
```bash
# é™ä½æ˜¾å­˜å ç”¨
export EMBEDDING_GPU_MEMORY_UTILIZATION=0.3

# æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹
export EMBEDDING_MODEL_NAME=Qwen/Qwen3-Embedding-0.6B
```

### 3. æ¨¡å‹ä¸‹è½½å¤±è´¥

**è§£å†³**:
```bash
# æ–¹å¼1: è®¾ç½® HuggingFace é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æ–¹å¼2: æ‰‹åŠ¨ä¸‹è½½åä½¿ç”¨æœ¬åœ°è·¯å¾„
export EMBEDDING_MODEL_PATH=/path/to/downloaded/model
```

### 4. æ²¡æœ‰GPU

æœåŠ¡å¯ä»¥åœ¨ CPU ä¸Šè¿è¡Œï¼Œä½†é€Ÿåº¦ä¼šå¾ˆæ…¢ï¼š

```bash
# CPUæ¨¡å¼ä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œæ— éœ€é¢å¤–é…ç½®
# ä½†æ¨èä½¿ç”¨æ›´å°çš„æ¨¡å‹ä»¥æå‡é€Ÿåº¦
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹é‡å¤„ç†

âŒ **é¿å…**ï¼šå¤šæ¬¡å•ä¸ªè¯·æ±‚
```python
for text in texts:
    response = requests.post(url, json={"input": text})
```

âœ… **æ¨è**ï¼šæ‰¹é‡è¯·æ±‚
```python
response = requests.post(url, json={"input": texts})
```

### 2. æé«˜æ˜¾å­˜åˆ©ç”¨ç‡

å¦‚æœæœ‰è¶³å¤Ÿæ˜¾å­˜ï¼Œå¯ä»¥æé«˜åˆ©ç”¨ç‡ï¼š

```bash
# é»˜è®¤0.4ï¼Œå¯ä»¥æé«˜åˆ°0.8-0.9
export EMBEDDING_GPU_MEMORY_UTILIZATION=0.8
```

### 3. ä½¿ç”¨è¿æ¥æ± 

```python
import requests

# åˆ›å»ºsessionå¤ç”¨è¿æ¥
session = requests.Session()

# å¤šæ¬¡è¯·æ±‚å¤ç”¨è¿æ¥
for batch in batches:
    response = session.post(url, json={"input": batch})
```

## ğŸ“š æ›´å¤šæ–‡æ¡£

- [API æ–‡æ¡£](API.md) - å®Œæ•´çš„ API æ¥å£è¯´æ˜
- [README](README.md) - è¯¦ç»†çš„åŠŸèƒ½ä»‹ç»å’Œæ¶æ„è¯´æ˜
- [.env.example](.env.example) - æ‰€æœ‰é…ç½®å‚æ•°è¯´æ˜

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚é‡åˆ°é—®é¢˜ï¼š

1. æ£€æŸ¥æœåŠ¡æ—¥å¿—è¾“å‡º
2. è®¿é—® `/health` ç«¯ç‚¹æŸ¥çœ‹æœåŠ¡çŠ¶æ€
3. æŸ¥çœ‹ [å¸¸è§é—®é¢˜](#-å¸¸è§é—®é¢˜) ç« èŠ‚
4. å‚è€ƒ [API æ–‡æ¡£](API.md) äº†è§£è¯¦ç»†ç”¨æ³•

## ğŸ¯ ä¸‹ä¸€æ­¥

- [ ] æµ‹è¯•æœåŠ¡æ˜¯å¦æ­£å¸¸å·¥ä½œ
- [ ] é›†æˆåˆ° rag-llm é¡¹ç›®
- [ ] æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´é…ç½®
- [ ] éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼ˆå¯é€‰ï¼‰
